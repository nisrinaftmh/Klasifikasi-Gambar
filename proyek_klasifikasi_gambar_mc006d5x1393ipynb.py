# -*- coding: utf-8 -*-
"""Proyek Klasifikasi Gambar_MC006D5X1393ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aYmzAHntWqrq4vX07_VErJaHnhP9OjxK

# Proyek Klasifikasi Gambar Assignment 4

Nama : Nisrina Fatimah Parisya

ID Cohort : MC006D5X1393

**Import Library**
"""

import os, shutil
import zipfile
import random
from random import sample
import shutil
from shutil import copyfile
import pathlib
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import pandas as pd
from tqdm.notebook import tqdm as tq
import cv2
from PIL import Image
import skimage
from skimage import io
from skimage.transform import resize
from skimage.transform import rotate, AffineTransform, warp
from skimage import img_as_ubyte
from skimage.exposure import adjust_gamma
from skimage.util import random_noise
import keras
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
import tensorflow as tf
from tensorflow.keras import Model, layers
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator, img_to_array, load_img
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.layers import InputLayer, Conv2D, SeparableConv2D, MaxPooling2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.applications import MobileNet
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
# Mencetak versi TensorFlow yang sedang digunakan
print(tf.__version__)

"""**Load Dataset**"""

!pip install -q kaggle

from google.colab import files
files.upload()

!chmod 600 /content/kaggle.json

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/

# Download dataset
!kaggle datasets download -d gunavenkatdoddi/eye-diseases-classification

# Unzip hasil download
!unzip eye-diseases-classification.zip -d eye_disease_data

"""## Gabungan Data"""

cataract_dir = '/content/eye_disease_data/dataset/cataract'
diabetic_dir = '/content/eye_disease_data/dataset/diabetic_retinopathy'
glaucoma_dir = '/content/eye_disease_data/dataset/glaucoma'
normal_dir = '/content/eye_disease_data/dataset/normal'

# Direktori baru untuk dataset gabungan
combined_dir = "/content/eye_disease_data/dataset/gabunganData"

# Buat direktori baru untuk dataset gabungan
os.makedirs(combined_dir, exist_ok=True)

def salin_semua_gambar(folder_asal, label):
    path_label = os.path.join(combined_dir, label)
    os.makedirs(path_label, exist_ok=True)

    for nama_file in os.listdir(folder_asal):
        path_file = os.path.join(folder_asal, nama_file)
        if os.path.isfile(path_file):
            shutil.copy2(path_file, os.path.join(path_label, nama_file))

# Jalankan untuk setiap kelas
salin_semua_gambar(cataract_dir, 'cataract')
salin_semua_gambar(diabetic_dir, 'diabetic_retinopathy')
salin_semua_gambar(glaucoma_dir, 'glaucoma')
salin_semua_gambar(normal_dir, 'normal')

print("Semua gambar berhasil disalin ke folder gabungan.")

"""## Dataset Checking"""

#Untuk menyimpan gambar
eyedisease_pict = {}

path = "/content/eye_disease_data/dataset/gabunganData"
for disease_type in os.listdir(path):
    disease_folder = os.path.join(path, disease_type)
    if os.path.isdir(disease_folder):
        eyedisease_pict[disease_type] = os.listdir(disease_folder)


fig, axs = plt.subplots(len(eyedisease_pict), 6, figsize=(8, 8))
for row_index, (disease_type, image_list) in enumerate(eyedisease_pict.items()):
    selected_images = np.random.choice(image_list, 6, replace=False)

    for col_index, image_name in enumerate(selected_images):
        image_path = os.path.join(path, disease_type, image_name)
        img = Image.open(image_path).convert("L")  # Ubah ke grayscale
        axs[row_index, col_index].imshow(img, cmap='gray')
        axs[row_index, col_index].set(xlabel=disease_type, xticks=[], yticks=[])

fig.tight_layout()
plt.show()

"""## Plot Distribusi"""

eyedisease_pict = '/content/eye_disease_data/dataset/gabunganData'

distribusi_disease =[]
label = []
full_path = []

for path, _, files in os.walk(eyedisease_pict):
    for name in files:
        full_path.append(os.path.join(path, name))
        label.append(os.path.basename(path))
        distribusi_disease.append(name)
eye_df = pd.DataFrame({
    "file_path": full_path,
    "file_name": distribusi_disease,
    "label": label
})

# Visualisasi distribusi label
plt.figure(figsize=(6, 6))
sns.countplot(x="label", data=eye_df)
plt.title("Distribusi Gambar Jenis Penyakit Mata di Dataset Eye Disease")
plt.xlabel("Jenis Penyakit Mata")
plt.ylabel("Jumlah Gambar")
plt.xticks(rotation=15)
plt.tight_layout()
plt.show()

"""## Data Splitting"""

usePath ='/content/eye_disease_data/dataset/gabunganData'

namafile = []
jenisPenyakit = []
paths = []

for root, subdirs, files in os.walk(usePath):
    for file in files:
        paths.append(os.path.join(root, file))
        jenisPenyakit.append(root.split('/')[-1])
        namafile.append(file)

# Membuat DataFrame dengan informasi file gambar
eye_disease_df = pd.DataFrame({
    "path": paths,
    "file_name": namafile,
    "Jenis Penyakit": jenisPenyakit
})

# Melihat jumlah gambar
eye_disease_df.groupby('Jenis Penyakit').size()

x = eye_disease_df['path']
y = eye_disease_df['Jenis Penyakit']

# Split 80% untuk train+val dan 20% untuk test
x_temp, x_test, y_temp, y_test = train_test_split(x, y, test_size=0.2, random_state=300)

# Dari 80% tadi, split lagi menjadi 75% train dan 25% val
# (0.25 * 0.8 = 0.2 jadi val 20% dari total data)
x_train, x_val, y_train, y_val = train_test_split(x_temp, y_temp, test_size=0.25, random_state=300)

print(f"Train: {len(x_train)}")
print(f"Validation: {len(x_val)}")
print(f"Test: {len(x_test)}")

train_df = pd.DataFrame({'path':x_train,'labels':y_train,'set':'train'})
test_df = pd.DataFrame({'path':x_test,'labels':y_test,'set':'test'})
val_df = pd.DataFrame({'path':x_val,'labels':y_val,'set':'val'})

df_allData = pd.concat([train_df,test_df,val_df],ignore_index=True)
print(df_allData.groupby(['set', 'labels']).size(), '\n')

print(df_allData.sample(5))

dataSource_path = '/content/eye_disease_data/dataset/gabunganData'
dataset_path = 'FIX_FINAL_DATASET'

#cek directory
if not os.path.exists(dataset_path):
    os.makedirs(dataset_path)

for index, row in df_allData.iterrows():
    source_path = row['path']
    destination_dir = os.path.join(dataset_path, row['set'], row['labels'])

    os.makedirs(destination_dir, exist_ok=True)

    # tentuin path yg dituju
    destination_path = os.path.join(destination_dir, os.path.basename(source_path))

    # cek source file
    if os.path.exists(source_path):
        shutil.copy2(source_path, destination_path)
    else:
        print(f"File not found: {source_path}")

TRAIN_DIR = "FIX_FINAL_DATASET/train/"
TEST_DIR = "FIX_FINAL_DATASET/test/"

# training set
train_cataract = os.path.join(TRAIN_DIR, 'cataract')
train_retinopathy = os.path.join(TRAIN_DIR, 'diabetic_retinopathy')
train_glaucoma = os.path.join(TRAIN_DIR, 'glaucoma')
train_normal = os.path.join(TRAIN_DIR, 'normal')

# test set
test_cataract = os.path.join(TEST_DIR, 'cataract')
test_retinopathy = os.path.join(TEST_DIR, 'diabetic_retinopathy')
test_glaucoma = os.path.join(TEST_DIR, 'glaucoma')
test_normal = os.path.join(TEST_DIR, 'normal')


# output
print("Training set:")
print(" - Cataract: ", len(os.listdir(train_cataract)))
print(" - Diabetic Retinopathy: ", len(os.listdir(train_retinopathy)))
print(" - Glaucoma: ", len(os.listdir(train_glaucoma)))
print(" - Normal: ", len(os.listdir(train_normal)))

print("\nTest set:")
print(" - Cataract: ", len(os.listdir(test_cataract)))
print(" - Diabetic Retinopathy: ", len(os.listdir(test_retinopathy)))
print(" - Glaucoma: ", len(os.listdir(test_glaucoma)))
print(" - Normal: ", len(os.listdir(test_normal)))

TRAIN_DIR = "FIX_FINAL_DATASET/train/"
VAL_DIR = "FIX_FINAL_DATASET/val/"
TEST_DIR = "FIX_FINAL_DATASET/test/"

train_datagen = ImageDataGenerator(rescale=1./255)
val_datagen = ImageDataGenerator(rescale=1./255)
test_datagen = ImageDataGenerator(rescale=1./255)

# Generator untuk training
train_generator = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(150, 150),
    color_mode="rgb",
    batch_size=32,
    class_mode='categorical',  # multiclass classification
    shuffle=True
)

# Generator untuk validation
validation_generator = val_datagen.flow_from_directory(
    VAL_DIR,
    target_size=(150, 150),
    color_mode="rgb",
    batch_size=32,
    class_mode='categorical',
    shuffle=False
)

# Generator untuk testing
test_generator = test_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(150, 150),
    color_mode="rgb",
    batch_size=1,
    class_mode='categorical',
    shuffle=False
)

"""## Modeling"""

model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),
    MaxPooling2D(2,2),

    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(2,2),

    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),  # prevent overfitting
    Dense(4, activation='softmax')  # multiclass (4 kelas)
])

model.compile(optimizer=tf.keras.optimizers.RMSprop(),
              loss='categorical_crossentropy',  # multiclass
              metrics=['accuracy'])

# Summary
model.summary()

from sklearn.utils.class_weight import compute_class_weight
import numpy as np

labels = train_generator.classes
class_names = list(train_generator.class_indices.keys())

# Hitung bobot
class_weights_array = compute_class_weight(
    class_weight='balanced',
    classes=np.unique(labels),
    y=labels
)

class_weights = {i: w for i, w in enumerate(class_weights_array)}

history = model.fit(
    train_generator,
    epochs=30,
    validation_data=validation_generator,
    class_weight=class_weights
)

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(len(acc))

plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(epochs, acc, 'r', label='Training Accuracy')
plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(epochs, loss, 'r', label='Training Loss')
plt.plot(epochs, val_loss, 'b', label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

# Prediksi pada test set
test_generator.reset()  # Reset test generator untuk memastikan dimulai dari awal

# Prediksi kelas
preds = model.predict(test_generator, verbose=0)
preds = preds.copy()
preds = preds.argmax(axis=1)

# Confusion Matrix
cm = pd.DataFrame(
    data=confusion_matrix(test_generator.classes, preds, labels=[0, 1, 2, 3]),
    index=["Actual Cataract", "Actual Diabetic Retinopathy", "Actual Glaucoma", "Actual Normal"],
    columns=["Predicted Cataract", "Predicted Diabetic Retinopathy", "Predicted Glaucoma", "Predicted Normal"]
)

# Visualisasi Confusion Matrix
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")

#  Classification Report
print("\nClassification Report:")
print(classification_report(
    y_true=test_generator.classes,
    y_pred=preds,
    target_names=['Cataract', 'Diabetic Retinopathy', 'Glaucoma', 'Normal'],
    digits=4
))

# SAVEDMODEL
saved_model_dir = 'saved_model_eye/'
model.export(saved_model_dir)
print(f"> SavedModel tersimpan di: {saved_model_dir}/")

# TF-LITE
# converter dari SavedModel
converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
tflite_model = converter.convert()

tflite_path = 'tflite/eye_model.tflite'
with open(tflite_path, 'wb') as f:
    f.write(tflite_model)

# Simpan label.txt
labels = ['cataract', 'diabetic retinopathy', 'glaucoma', 'normal']
label_path = 'tflite/label.txt'
with open(label_path, 'w') as f:
    for label in labels:
        f.write(label + '\n')

!pip install tensorflowjs

import tensorflowjs as tfjs
#TFJS
tfjs_target_dir = 'tfjs_model/'
tfjs.converters.save_keras_model(model, tfjs_target_dir)
print(f"> TFJS model tersimpan di folder: {tfjs_target_dir}/")

import shutil
shutil.make_archive('saved_model_eye', 'zip', 'saved_model_eye')

import shutil
shutil.make_archive('tfjs_model', 'zip', 'tfjs_model')

import shutil
shutil.make_archive('tflite', 'zip', 'tflite')

!pip freeze | grep 'numpy\|pandas\|matplotlib\|seaborn\|tqdm\|opencv\|Pillow\|scikit-image\|scikit-learn\|tensorflow' > requirements.txt
!cat requirements.txt

# Commented out IPython magic to ensure Python compatibility.
# %%writefile README.md
# 
# # ğŸ§  Proyek Klasifikasi Penyakit Mata Berdasarkan Gambar Retina
# Proyek ini merupakan bagian dari pemenuhan tugas modul Pengembangan Machine Learning, Saya mengambil contoh studi kasus dimana modeling ini nantinya digunakan untuk
# mengklasifikasikan penyakit mata yang diambil dari dataset kaggle 'https://www.kaggle.com/datasets/gunavenkatdoddi/eye-diseases-classification'.
# 
# ## ğŸ©º Deskripsi Dataset
# 
# Dataset ini terdiri dari gambar retina manusia yang diklasifikasikan ke dalam empat kategori:
# - **Katarak**
# - **Glaukoma**
# - **Retinopati**
# - **Normal**
# 
# Tujuan dari proyek ini adalah membangun model klasifikasi berbasis CNN (Convolutional Neural Network) untuk mengidentifikasi jenis penyakit berdasarkan citra retina.
# 
# ---
# 
# ## ğŸ“ Isi Notebook
# 
# a. **Import Library**
#    - Inisiasi seluruh dependensi dan library yang digunakan, seperti TensorFlow, NumPy, Matplotlib, dll.
# 
# b. **Eksplorasi Dataset**
#    - Pemeriksaan struktur dataset.
#    - Visualisasi distribusi jumlah gambar untuk setiap label/kategori.
# 
# c. **Preprocessing**
#    - Resize gambar ke dimensi tertentu.
# 
#    - Pembagian dataset menjadi tiga bagian: **train**, **validation**, dan **test**.
# 
# d. **Pembuatan dan Pelatihan Model**
#    - Membangun model CNN sederhana dengan 32 neuron pada layer awal.
#    - Pelatihan model menggunakan data latih dan validasi.
# 
# e. **Evaluasi Model**
#    - Evaluasi performa model menggunakan data uji.
#    - Menampilkan akurasi dan loss, serta metrik tambahan jika diperlukan.
# 
# ---
# 
# ## â–¶ï¸ Cara Menjalankan Notebook
# - Pastikan semua library dan dependensi yang akan digunakan sudah terpasang
# - Jalankan kode di dalam sel notebook proyek ini
# - Selalu cek kembali apakah path yang telah dibuat tersinkronasi dengan dataset dan folder yang tersedia